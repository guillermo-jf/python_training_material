{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1GKidOoqs9q-vchtRfcfRM7wanrXlUmi8","timestamp":1704787579390}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#**Llama 2**"],"metadata":{"id":"4bKQIsIq-d8y"}},{"cell_type":"markdown","source":["The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases."],"metadata":{"id":"PnV5UC7A2vBZ"}},{"cell_type":"markdown","source":[" It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety."],"metadata":{"id":"AC41zK5l3Abp"}},{"cell_type":"markdown","source":["For more information about LLama, check: Hugging Face\n","[Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)"],"metadata":{"id":"4nobX9E83PjQ"}},{"cell_type":"markdown","source":["This notebook uses `GGML`, a C library for machine learning that facilitates the distribution of large language models (LLMs). GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size."],"metadata":{"id":"0K4QuEDH4CbY"}},{"cell_type":"markdown","source":["#  Quantized Models from the Hugging Face Community"],"metadata":{"id":"3YC846SH5DOK"}},{"cell_type":"markdown","source":["The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n","\n","There are several variations available, but the ones that interest us are based on the GGLM library.\n","\n","We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n","\n","\n","\n","In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."],"metadata":{"id":"0TD82wis5LGA"}},{"cell_type":"markdown","source":["#**Step 1: Install All the Required Packages**"],"metadata":{"id":"YQZBmz7I5neU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0avf7xx2lcj"},"outputs":[],"source":["# GPU llama-cpp-python\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n","!pip install huggingface_hub\n","!pip install llama-cpp-python==0.1.78\n","!pip install numpy==1.23.4"]},{"cell_type":"code","source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n","model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"],"metadata":{"id":"qJ90LnMv54Y-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Step 2: Import All the Required Libraries**"],"metadata":{"id":"6lOmpKB36RJh"}},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n"],"metadata":{"id":"Ak3ZtGjM6Wdp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llama_cpp import Llama\n"],"metadata":{"id":"85XOzmui6rGN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Step 3: Download the Model**"],"metadata":{"id":"haAb9kNm6J9n"}},{"cell_type":"code","source":["model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"],"metadata":{"id":"qBgdGV4b6MxG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Step 4: Loading the Model**"],"metadata":{"id":"VQ6OYnI46kKq"}},{"cell_type":"code","source":["# GPU\n","lcpp_llm = None\n","lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=2, # CPU cores\n","    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n","    )"],"metadata":{"id":"irftToUj6aWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See the number of layers in GPU\n","lcpp_llm.params.n_gpu_layers"],"metadata":{"id":"YG4Pylz662At"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Step 5: Create a Prompt Template**\n","\n","Edit this prompt to interact with the LLM with different inputs. The prompt adds some context (\"you are a helpful...\") to improve the quality of the responses generated by the model.\n","\n","For practice, keep editing this prompt and try different inputs."],"metadata":{"id":"iE-M307R6_pT"}},{"cell_type":"code","source":["prompt = \"Describe the best metrics to evaluation a ML classification model\"\n","\n","prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n","\n","USER: {prompt}\n","\n","ASSISTANT:\n","'''"],"metadata":{"id":"RfzwELMC7Dyg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Step 6: Generating the Response**\n","\n","This line populates a variable 'response' with the output generated by the LLM"],"metadata":{"id":"aT8pg6zt7QzA"}},{"cell_type":"code","source":["response=lcpp_llm(prompt=prompt_template, max_tokens=2048, temperature=0.5, top_p=0.95,\n","                  repeat_penalty=1.2, top_k=150,\n","                  echo=True)"],"metadata":{"id":"0aF0qWUJ7OPK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Display the value of the response."],"metadata":{"id":"ElYvCPRRYotl"}},{"cell_type":"code","source":["print(response)"],"metadata":{"id":"jlJ1JgR68DDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(response[\"choices\"][0][\"text\"])"],"metadata":{"id":"Qona58gX8oAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N4uMV0zF8pQt"},"execution_count":null,"outputs":[]}]}